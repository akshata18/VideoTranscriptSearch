(Refer Slide Time: 26:04) 

 Let us move on and let us talk about syntax analysis. The lexical analyzer returns these tokens. The same assignment statement that we considered before – id assign id multop fconst addop iconst; these are all our tokens. fconst is the floating point constant and iconst is the integer constant. These are fed to the syntax analyzer. The syntax analyzer make sure that the assignment statement is indeed correct in syntax. In other words, there is an identifier on the left side of an assignment, there is an identifier on the right side of an assignment followed by an expression or whatever operator, and so on and so forth. 

In general, the programming language constructs are complex. There is if then else, there is fall loop and so on and so forth. The lexical analyzer does not worry about such constructs. It simply returns tokens for most of these constructs. For example, if it is if 

then else, it is then safe. Then, for the entire expression, it returns a number of tokens followed by then and followed by the number of tokens for statement, and so on and so forth. 

The syntax analyzer would look at the stream of tokens that is coming into it. It uses a context-free grammar to check whether the rules are all appropriately satisfied and then it constructs what is known as a syntax tree. Here is a syntax tree (Refer Slide Time: 27:41) for the assignment statement. The assignment has left child as an identifier, the right child as a plus operator. The plus operator has left child as a star and its right child is the constant. The star operator has identifier on the left-hand side and then the constant 1.8 on the right-hand side. So, such syntax trees are produced and fed to the semantic analyzer. (Refer Slide Time: 28:15) 

 Syntax analyzers can be generated automatically from context-free grammar specifications. As I said, context-free grammar is the basis of parsing. A pushdown automaton is constructed from such a context-free grammar specification and then it is fed a sequence of tokens and it containers There are many tools, which can do this. For example, ANTLR is a tool which takes LL 1 context-free grammar and produces a top-down parser. YACC and Bison; YACC is a tool with unix and bison is the corresponding tool available from GNU. These take LALR 1 form of context-free grammar and produces a parser for such grammars. These parsers are all deterministic pushdown automaton, but the main problem with these parsers are – they cannot handle any semantic features of programming languages, which are known as context sensitivity features of a programming language. For example: If you have variables in the program, obviously you would have number of them; to check whether your variables have been declared before they are used in the program is a context sensitive feature. You really cannot check whether it is possible or such a declaration exists. Secondly, whether the left and right sides of an assignment match; it is something that we really cannot check in a context-free grammar and using a pushdown automaton. The reason is – for a context-free grammar and a pushdown automaton, which is produced by it, whether the left hand side is an array name or whether it is a simple integer name is not known. That information cannot be captured in a context-free grammar. Therefore, checking whether the right-hand side also happens to be an array of values or whether it is simple arithmetic expression producing an integer value, cannot be checked by the same context-free grammar. For doing this, we need special types of grammars called attribute grammar and we will see a simple example very soon. Third example of a context sensitivity feature is regarding parameter. You would have a number parameters in a function and you would actually putdown the declaration of the function and its parameters and then call the function with the actual parameter list. Whether the types of parameters in the usage match the types of parameters in the declaration is a context sensitive feature. This cannot be captured in a context-free grammar and therefore, we need the next phase of compiler called the semantic analyzer phase. A syntax tree as I said, will be produced as the output from a syntax analyzer, but I must add that this does not always happen. In some cases, if the entire compiler is a one-pass compiler; in other words, it produces even the machine code in one-pass, it is not necessary to produce the syntax tree explicitly. However, if there are language constructs such as in C plus plus, which says – you can use the variables and put the declaration elsewhere, perhaps much later; that is possible in the class in C plus plus; such constructs cannot be validated semantically in a single pass. We need to produce the syntax tree decorated with some of the semantic information available from the program and pass this entire thing to the semantic analyzer for validation. So, that is really what is we need to see next. 




